# -*- coding: utf-8 -*-
"""fake news classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19LIJzwQYSYpdTT6Vg8hS_29P6QmGC4p5
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

import pandas as pd
fake_df = pd.read_csv('Fake.csv')
real_df = pd.read_csv('True.csv')

fake_df['label'] = 1
real_df['label'] = 0

df = pd.concat([fake_df, real_df], ignore_index=True)

df = df.sample(frac=1, random_state=42).reset_index(drop=True)

import re

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'[^\w\s]', '', text)
    return text

df['text'] = df['text'].apply(clean_text)

# 7. Tokenization and padding
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

max_words = 10000
max_len = 300

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(df['text'])
sequences = tokenizer.texts_to_sequences(df['text'])
X = pad_sequences(sequences, maxlen=max_len)
y = df['label'].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional

model = Sequential([
    Embedding(max_words, 64, input_length=max_len),
    Bidirectional(LSTM(64, return_sequences=True)),
    Dropout(0.5),
    Bidirectional(LSTM(32)),
    Dense(32, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

def predict_news(text):
    # Clean and preprocess
    import re
    text_clean = re.sub(r'[^\w\s]', '', text.lower())
    # Tokenize and pad
    seq = tokenizer.texts_to_sequences([text_clean])
    padded = pad_sequences(seq, maxlen=max_len)
    # Predict
    pred = model.predict(padded)[0][0]
    if pred > 0.5:
        return "Fake News"
    else:
        return "Real News"

# Code Generated by Sidekick is for learning and experimentation purposes only.
import numpy as np
import pandas as pd

# Get predictions
y_pred_probs = model.predict(X_test)
y_pred_labels = (y_pred_probs > 0.5).astype("int32").flatten()

# Get the original articles (from your split)
test_texts = df.iloc[X_test.index]['text'] if hasattr(X_test, 'index') else None

# If you used train_test_split, you can get the indices like this:
_, X_test_indices = train_test_split(df.index, test_size=0.2, random_state=42)
test_texts = df.iloc[X_test_indices]['text']

# Build a DataFrame to display results
results_df = pd.DataFrame({
    'Article': test_texts,
    'True Label': y_test,
    'Predicted Label': y_pred_labels
})

# Map numeric labels to readable text
results_df['True Label'] = results_df['True Label'].map({0: 'Real News', 1: 'Fake News'})
results_df['Predicted Label'] = results_df['Predicted Label'].map({0: 'Real News', 1: 'Fake News'})

# Show first few results
print(results_df.head(10))

def predict_news(text):
    import re
    text_clean = re.sub(r'[^\w\s]', '', text.lower())
    seq = tokenizer.texts_to_sequences([text_clean])
    padded = pad_sequences(seq, maxlen=max_len)
    pred = model.predict(padded)[0][0]
    return "Fake News" if pred > 0.5 else "Real News"

# Try it out
user_input = "NASA discovers water on Mars."
print(f"Prediction: {predict_news(user_input)}")

from google.colab import files
files.download('fake_news_model.h5')

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Example: train a simple model
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['text'])
y = df['label']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = LogisticRegression()
clf.fit(X_train, y_train)

import pickle

with open('fake_news_clf.pkl', 'wb') as f:
    pickle.dump(clf, f)

from google.colab import files
files.download('fake_news_clf.pkl')

!pip install streamlit

# Code Generated by Sidekick is for learning and experimentation purposes only.
import pickle
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

pip install streamlit

# Code Generated by Sidekick is for learning and experimentation purposes only.
from google.colab import files
files.download('fake_news_model.h5')
files.download('tokenizer.pickle')

!pip install streamlit tensorflow

from google.colab import files
uploaded = files.upload()

# Code Generated by Sidekick is for learning and experimentation purposes only.
app_code = '''
import streamlit as st
import tensorflow as tf
import pickle
from tensorflow.keras.preprocessing.sequence import pad_sequences
import re

# Load the trained model
model = tf.keras.models.load_model('fake_news_model.h5')

# Load the tokenizer
with open('tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)

# Set max_len to the value used during training
max_len = 200  # Update if needed

def preprocess_text(text):
    # Use double backslashes to avoid escape warnings
    text_clean = re.sub("[^\\w\\s]", "", text.lower())
    seq = tokenizer.texts_to_sequences([text_clean])
    padded = pad_sequences(seq, maxlen=max_len)
    return padded

def predict_news(text):
    padded = preprocess_text(text)
    pred = model.predict(padded)[0][0]
    return "Fake News" if pred > 0.5 else "Real News"

st.title("Fake News Classifier")

user_input = st.text_area("Enter news headline or article:")

if st.button("Predict"):
    if user_input.strip() == "":
        st.warning("Please enter some text.")
    else:
        prediction = predict_news(user_input)
        st.success(f"Prediction: {prediction}")
'''

with open('streamlit_app.py', 'w') as f:
    f.write(app_code)

# Code Generated by Sidekick is for learning and experimentation purposes only.
!pip install pyngrok
from pyngrok import ngrok

# Paste your authtoken below (replace with your actual token)
authtoken = "32dqC4cSJeuoFRTgrzur2WZrNzt_4wq918Mg6bD5WRZuFZ85F"
ngrok.set_auth_token(authtoken)

# Code Generated by Sidekick is for learning and experimentation purposes only.
import subprocess
from pyngrok import ngrok

# Start Streamlit app in the background
streamlit_process = subprocess.Popen(['streamlit', 'run', 'streamlit_app.py'])

# Open a tunnel to the Streamlit app (use addr, not port)
public_url = ngrok.connect(addr="8501")
print('Streamlit app URL:', public_url)